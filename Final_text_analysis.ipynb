{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install syllapy textstat sumy pyngrok streamlit pymupdf docx2txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFpZK1L1mAr8",
        "outputId": "549fcb33-d651-4bf2-8d9c-0e9a9a8ecdac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syllapy\n",
            "  Downloading syllapy-0.7.2-py3-none-any.whl.metadata (854 bytes)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.1.0)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.28.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading syllapy-0.7.2-py3-none-any.whl (24 kB)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, breadability, docopt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=a5dbc32179a314fa1495aca35b44cbc9d5de1397efa3a9b4471cec1a99d9b17e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0e/7a/3094a4ceefe657bff7e12dd9592a9d5b6487ef4338ace0afa6\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=a855255f2e945e53b28006d632b3e2cab78819939a78a8702588cc1b0b2292ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=24111bc03ace3ce819b7b95241bacebc8328d014ee484865a81b6a040d5942a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docx2txt breadability docopt\n",
            "Installing collected packages: docx2txt, docopt, watchdog, syllapy, pyphen, pyngrok, pymupdf, pycountry, breadability, sumy, pydeck, cmudict, textstat, streamlit\n",
            "Successfully installed breadability-0.1.20 cmudict-1.0.32 docopt-0.6.2 docx2txt-0.8 pycountry-24.6.1 pydeck-0.9.1 pymupdf-1.25.3 pyngrok-7.2.3 pyphen-0.17.2 streamlit-1.42.2 sumy-0.11.0 syllapy-0.7.2 textstat-0.7.5 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nltkdata/gutenberg\")\n",
        "\n",
        "# Define the desired path\n",
        "desired_path = '/content/dataset'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(desired_path, exist_ok=True)\n",
        "\n",
        "# Move the dataset to the desired folder\n",
        "shutil.move(path, desired_path)\n",
        "\n",
        "print(f\"Dataset saved to {desired_path}\")\n"
      ],
      "metadata": {
        "id": "tHnvTkj6rE-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03607850-4eb5-4b19-f672-306c37c3153d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nltkdata/gutenberg?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.13M/4.13M [00:00<00:00, 6.06MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.io as pio  # For setting the renderer\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZigCfDCYa7vk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import fitz  # For PDF extraction\n",
        "import docx2txt  # For DOCX extraction\n",
        "\n",
        "# -------------------------------\n",
        "# Page Configuration\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"Advanced Text Analysis Dashboard\", layout=\"wide\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');\n",
        "    body, html {\n",
        "        font-family: 'Roboto', sans-serif;\n",
        "    }\n",
        "    /* Vibrant colorful background gradients */\n",
        "    .reportview-container {\n",
        "        background: linear-gradient(135deg, #f6d365, #fda085);\n",
        "    }\n",
        "    .sidebar .sidebar-content {\n",
        "        background: linear-gradient(135deg, #a1c4fd, #c2e9fb);\n",
        "    }\n",
        "    /* Colorful button styling */\n",
        "    .stButton>button {\n",
        "        background-color: #ff6f61;\n",
        "        color: white;\n",
        "        padding: 10px 20px;\n",
        "        border: none;\n",
        "        border-radius: 10px;\n",
        "        font-weight: bold;\n",
        "        transition: background-color 0.3s ease;\n",
        "    }\n",
        "    .stButton>button:hover {\n",
        "        background-color: #ff3b2e;\n",
        "    }\n",
        "    /* Header styling */\n",
        "    h1 {\n",
        "        color: #333;\n",
        "        text-align: center;\n",
        "        font-size: 3em;\n",
        "        margin-bottom: 20px;\n",
        "        text-shadow: 2px 2px 5px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    h2, h3 {\n",
        "        color: #333;\n",
        "    }\n",
        "    /* Footer styling */\n",
        "    .footer {\n",
        "        position: fixed;\n",
        "        left: 0;\n",
        "        bottom: 0;\n",
        "        width: 100%;\n",
        "        background-color: #f6d365;\n",
        "        color: #333;\n",
        "        text-align: center;\n",
        "        padding: 10px 0;\n",
        "        font-size: 0.9em;\n",
        "        border-top: 1px solid #ccc;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "st.title(\"Advanced Text Analysis Dashboard\")\n",
        "\n",
        "# -------------------------------\n",
        "# Download NLTK Data\n",
        "# -------------------------------\n",
        "nltk.download('punkt')\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ===============================\n",
        "# Analysis Functions\n",
        "# ===============================\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    dataset_texts, file_names = [], []\n",
        "    if not os.path.exists(dataset_path):\n",
        "        st.warning(f\"Dataset path '{dataset_path}' not found. Skipping dataset comparison.\")\n",
        "        return dataset_texts, file_names\n",
        "    for file in os.listdir(dataset_path):\n",
        "        try:\n",
        "            with open(os.path.join(dataset_path, file), \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "                dataset_texts.append(f.read())\n",
        "                file_names.append(file)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Skipping {file}: {e}\")\n",
        "    return dataset_texts, file_names\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return ' '.join([p.get_text() for p in soup.find_all('p') if p.get_text()])\n",
        "    return \"\"\n",
        "\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiouy\"\n",
        "    num_vowels = 0\n",
        "    prev_char_was_vowel = False\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                num_vowels += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "    if word.endswith(\"e\") and num_vowels > 1:\n",
        "        num_vowels -= 1\n",
        "    return num_vowels if num_vowels > 0 else 1\n",
        "\n",
        "def compute_readability_scores(text):\n",
        "    import nltk\n",
        "    words = nltk.word_tokenize(text)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "    total_syllables = sum(count_syllables(word) for word in words if word.isalpha())\n",
        "\n",
        "    if total_sentences == 0 or total_words == 0:\n",
        "        fk_grade = 0\n",
        "    else:\n",
        "        fk_grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
        "\n",
        "    dale_chall_easy_words = set([\n",
        "        \"a\", \"about\", \"all\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"can\",\n",
        "        \"for\", \"if\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \"to\",\n",
        "        \"was\", \"with\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"this\", \"there\"\n",
        "    ])\n",
        "    difficult_words = [word for word in words if word.isalpha() and word.lower() not in dale_chall_easy_words]\n",
        "    difficult_word_percentage = (len(difficult_words) / total_words) * 100 if total_words > 0 else 0\n",
        "    average_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
        "    raw_score = 0.1579 * difficult_word_percentage + 0.0496 * average_sentence_length\n",
        "    if difficult_word_percentage > 5:\n",
        "        raw_score += 3.6365\n",
        "\n",
        "    return {\"Flesch_Kincaid_Grade\": fk_grade, \"Dale_Chall\": raw_score}\n",
        "\n",
        "def jacobs_semantic_complexity(text, method=\"word2vec\", word2vec_model=None):\n",
        "    import nltk\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import torch\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if len(sentences) < 2:\n",
        "        return {\"avg_similarity\": None, \"std_similarity\": None, \"similarities\": []}\n",
        "\n",
        "    embeddings = []\n",
        "    if method == \"bert\":\n",
        "        bert_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        bert_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        for sentence in sentences:\n",
        "            inputs = bert_tokenizer(\n",
        "                sentence,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                outputs = bert_model(**inputs)\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "            embeddings.append(embedding)\n",
        "    else:\n",
        "        if word2vec_model is None:\n",
        "            raise ValueError(\"A Word2Vec model must be provided when using method='word2vec'.\")\n",
        "        from gensim.utils import simple_preprocess\n",
        "        import numpy as np\n",
        "        for sentence in sentences:\n",
        "            words = simple_preprocess(sentence)\n",
        "            word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "            if word_vectors:\n",
        "                embeddings.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                embeddings.append(np.zeros(word2vec_model.vector_size))\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]\n",
        "        similarities.append(sim)\n",
        "    import numpy as np\n",
        "    avg_similarity = np.mean(similarities)\n",
        "    std_similarity = np.std(similarities)\n",
        "    return {\"avg_similarity\": avg_similarity, \"std_similarity\": std_similarity, \"similarities\": similarities}\n",
        "\n",
        "def plot_semantic_complexity(metrics):\n",
        "    import plotly.express as px\n",
        "    similarities = metrics.get(\"similarities\", [])\n",
        "    if not similarities:\n",
        "        st.info(\"Not enough sentences to compute semantic complexity.\")\n",
        "        return None\n",
        "    avg_similarity = metrics.get(\"avg_similarity\", 0)\n",
        "    fig = px.line(\n",
        "        x=list(range(len(similarities))),\n",
        "        y=similarities,\n",
        "        labels={'x': 'Sentence Pair Index', 'y': 'Cosine Similarity'},\n",
        "        title=\"Jacobs' Semantic Complexity (Sentence Similarities)\",\n",
        "        markers=True\n",
        "    )\n",
        "    fig.add_hline(y=avg_similarity, line_dash=\"dash\", line_color=\"red\",\n",
        "                  annotation_text=f\"Average Similarity: {avg_similarity:.2f}\",\n",
        "                  annotation_position=\"bottom right\")\n",
        "    return fig\n",
        "\n",
        "def analyze_emotions(text):\n",
        "    words = text.lower().split()\n",
        "    categories = {\n",
        "        \"Love\": [\"love\", \"affection\", \"joy\"],\n",
        "        \"Hate\": [\"hate\", \"anger\"],\n",
        "        \"Conflict\": [\"fight\", \"war\"]\n",
        "    }\n",
        "    return {emotion: sum(words.count(w) for w in words_list)\n",
        "            for emotion, words_list in categories.items()}\n",
        "\n",
        "from transformers import pipeline\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    return_all_scores=True,\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "def analyze_emotions_pretrained(text):\n",
        "    results = emotion_classifier(text, truncation=True, max_length=512)\n",
        "    if isinstance(results, list) and results and isinstance(results[0], list):\n",
        "        results = results[0]\n",
        "    emotions = {result['label']: result['score'] for result in results}\n",
        "    return emotions\n",
        "\n",
        "def analyze_emotions_over_time(text):\n",
        "    import nltk\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    possible_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
        "    emotions_over_time = {lbl: [] for lbl in possible_labels}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        results = emotion_classifier(sentence, truncation=True, max_length=512)\n",
        "        if isinstance(results, list) and results and isinstance(results[0], list):\n",
        "            results = results[0]\n",
        "        sent_emotions = {r['label']: r['score'] for r in results}\n",
        "        for lbl in possible_labels:\n",
        "            emotions_over_time[lbl].append(sent_emotions.get(lbl, 0.0))\n",
        "    return emotions_over_time\n",
        "\n",
        "def plot_emotions_over_time(emotions_dict):\n",
        "    import plotly.graph_objects as go\n",
        "    sentence_indices = list(range(len(next(iter(emotions_dict.values())))))\n",
        "    fig = go.Figure()\n",
        "    for emotion_label, scores in emotions_dict.items():\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=sentence_indices,\n",
        "            y=scores,\n",
        "            mode='lines+markers',\n",
        "            name=emotion_label\n",
        "        ))\n",
        "    fig.update_layout(\n",
        "        title=\"Emotion Changes Over Time\",\n",
        "        xaxis_title=\"Sentence Index\",\n",
        "        yaxis_title=\"Emotion Score\",\n",
        "        legend_title=\"Emotions\"\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "sentiment_analysis_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "def analyze_sentiment_pretrained(text):\n",
        "    results = sentiment_analysis_pipeline(text, truncation=True, max_length=512)\n",
        "    if isinstance(results, list) and results and isinstance(results[0], list):\n",
        "        results = results[0]\n",
        "    mapping = {\n",
        "        \"LABEL_0\": \"negative\",\n",
        "        \"LABEL_1\": \"neutral\",\n",
        "        \"LABEL_2\": \"positive\"\n",
        "    }\n",
        "    sentiment_scores = {mapping.get(result['label'], result['label']): result['score']\n",
        "                        for result in results}\n",
        "    return sentiment_scores\n",
        "\n",
        "def plot_sentiment_bar(sentiment_scores):\n",
        "    import plotly.express as px\n",
        "    desired_order = [\"negative\", \"positive\", \"neutral\"]\n",
        "    scores = [sentiment_scores.get(label, 0) for label in desired_order]\n",
        "    fig = px.bar(\n",
        "        x=desired_order,\n",
        "        y=scores,\n",
        "        labels={'x': 'Sentiment', 'y': 'Score'},\n",
        "        title='Overall Sentiment Distribution',\n",
        "        color=desired_order\n",
        "    )\n",
        "    fig.update_layout(xaxis_title=\"Sentiment\", yaxis_title=\"Score\")\n",
        "    return fig\n",
        "\n",
        "def track_sentiment_over_time(text):\n",
        "    import nltk\n",
        "    import plotly.express as px\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    net_sentiments = []\n",
        "    for sentence in sentences:\n",
        "        sentiment = analyze_sentiment_pretrained(sentence)\n",
        "        net = sentiment.get(\"positive\", 0) - sentiment.get(\"negative\", 0)\n",
        "        net_sentiments.append(net)\n",
        "    fig = px.line(\n",
        "        x=list(range(len(sentences))),\n",
        "        y=net_sentiments,\n",
        "        labels={'x': 'Sentence Index', 'y': 'Net Sentiment'},\n",
        "        title='Sentiment Changes Over Time',\n",
        "        markers=True\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "summarization_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "def generate_summary(text, max_length=130, min_length=30):\n",
        "    summary = summarization_pipeline(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "def train_word2vec(texts):\n",
        "    from gensim.utils import simple_preprocess\n",
        "    sentences = [simple_preprocess(text) for text in texts]\n",
        "    return Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def compute_similarity(text, dataset_texts, model):\n",
        "    import numpy as np\n",
        "    from gensim.utils import simple_preprocess\n",
        "    text_vector = np.mean([model.wv[w] for w in simple_preprocess(text) if w in model.wv], axis=0)\n",
        "    return [\n",
        "        cosine_similarity(\n",
        "            [text_vector],\n",
        "            [np.mean([model.wv[w] for w in simple_preprocess(doc) if w in model.wv], axis=0)]\n",
        "        )[0][0]\n",
        "        for doc in dataset_texts\n",
        "    ]\n",
        "\n",
        "def plot_word_cloud(text, max_words=100):\n",
        "    # Static word cloud\n",
        "    wc = WordCloud(width=800, height=400, background_color='white',\n",
        "                   max_words=max_words, colormap='viridis').generate(text)\n",
        "    fig = px.imshow(wc)\n",
        "    fig.update_layout(\n",
        "        title=f'Word Cloud (Top {max_words} Words)',\n",
        "        xaxis_visible=False,\n",
        "        yaxis_visible=False\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def animated_wordcloud(text, max_words=100):\n",
        "    \"\"\"\n",
        "    Generates an animated word cloud figure by creating frames with increasing number of words.\n",
        "    \"\"\"\n",
        "    # Set up the range and step for the number of words\n",
        "    step = max(5, int(max_words/10))\n",
        "    frames = []\n",
        "    for num in range(10, max_words+1, step):\n",
        "        wc = WordCloud(width=800, height=400, background_color='white',\n",
        "                       max_words=num, colormap='viridis').generate(text)\n",
        "        img = wc.to_array()\n",
        "        frames.append(go.Frame(data=[go.Image(z=img)], name=str(num)))\n",
        "    # Create the initial static word cloud with the smallest number of words\n",
        "    wc_initial = WordCloud(width=800, height=400, background_color='white',\n",
        "                           max_words=10, colormap='viridis').generate(text)\n",
        "    fig = go.Figure(\n",
        "        data=[go.Image(z=wc_initial.to_array())],\n",
        "        layout=go.Layout(\n",
        "            title=\"Animated Word Cloud\",\n",
        "            xaxis=dict(visible=False),\n",
        "            yaxis=dict(visible=False),\n",
        "            updatemenus=[{\n",
        "                \"type\": \"buttons\",\n",
        "                \"buttons\": [{\n",
        "                    \"label\": \"Play\",\n",
        "                    \"method\": \"animate\",\n",
        "                    \"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": True}, \"fromcurrent\": True}]\n",
        "                }]\n",
        "            }]\n",
        "        ),\n",
        "        frames=frames\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def plot_word_frequency(text, top_n=10):\n",
        "    import plotly.express as px\n",
        "    from collections import Counter\n",
        "    word_counts = Counter(text.split()).most_common(top_n)\n",
        "    fig = px.treemap(\n",
        "        names=[wc[0] for wc in word_counts],\n",
        "        parents=[\"Words\"] * len(word_counts),\n",
        "        values=[wc[1] for wc in word_counts],\n",
        "        title='Word Frequency'\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def plot_similarity(similarity_scores, file_names):\n",
        "    import plotly.express as px\n",
        "    fig = px.imshow(\n",
        "        [similarity_scores],\n",
        "        labels=dict(x=\"Dataset Files\", y=\"Input Text\", color=\"Similarity\"),\n",
        "        x=file_names,\n",
        "        y=['Input Text']\n",
        "    )\n",
        "    fig.update_layout(title='Text Similarity Heatmap', coloraxis_showscale=True)\n",
        "    return fig\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "def plot_tsne(model):\n",
        "    import plotly.express as px\n",
        "    words, vectors = zip(*[(word, model.wv[word]) for word in model.wv.index_to_key[:100]])\n",
        "    vectors_array = np.array(vectors)\n",
        "    tsne_results = TSNE(n_components=2, perplexity=15, random_state=42).fit_transform(vectors_array)\n",
        "    df = {\n",
        "        \"x\": tsne_results[:, 0],\n",
        "        \"y\": tsne_results[:, 1],\n",
        "        \"word\": words\n",
        "    }\n",
        "    fig = px.scatter(\n",
        "        df,\n",
        "        x=\"x\",\n",
        "        y=\"y\",\n",
        "        text=\"word\",\n",
        "        title=\"t-SNE Visualization of Word Embeddings\",\n",
        "        width=700,\n",
        "        height=600\n",
        "    )\n",
        "    fig.update_traces(textposition='top center', marker=dict(size=8, color='blue'))\n",
        "    return fig\n",
        "\n",
        "# ===============================\n",
        "# Streamlit App Layout & UI\n",
        "# ===============================\n",
        "\n",
        "# Sidebar: Input Options\n",
        "st.sidebar.header(\"Input Options\")\n",
        "input_option = st.sidebar.selectbox(\"Select Input Type\", [\"Enter Text\", \"Enter URL\", \"Upload File\"])\n",
        "\n",
        "# --- Input text, URL, or file ---\n",
        "if input_option == \"Enter Text\":\n",
        "    user_text = st.sidebar.text_area(\"Enter your text here (max 1000 words)\", height=200)\n",
        "    user_text = \" \".join(user_text.split()[:1000])\n",
        "elif input_option == \"Enter URL\":\n",
        "    url_input = st.sidebar.text_input(\"Enter a URL\")\n",
        "    if st.sidebar.button(\"Fetch URL\"):\n",
        "        with st.spinner(\"Fetching and processing URL...\"):\n",
        "            user_text = extract_text_from_url(url_input)\n",
        "        user_text = \" \".join(user_text.split()[:1000])\n",
        "    else:\n",
        "        user_text = \"\"\n",
        "elif input_option == \"Upload File\":\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Upload PDF or DOCX file\", type=[\"pdf\", \"docx\"])\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.name.endswith(\".pdf\"):\n",
        "            try:\n",
        "                file_bytes = uploaded_file.read()\n",
        "                import fitz\n",
        "                with fitz.open(stream=file_bytes, filetype=\"pdf\") as doc:\n",
        "                    text = \"\"\n",
        "                    for page in doc:\n",
        "                        text += page.get_text()\n",
        "                user_text = text\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing PDF file: {e}\")\n",
        "                user_text = \"\"\n",
        "        elif uploaded_file.name.endswith(\".docx\"):\n",
        "            import docx2txt\n",
        "            try:\n",
        "                user_text = docx2txt.process(uploaded_file)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing DOCX file: {e}\")\n",
        "                user_text = \"\"\n",
        "        else:\n",
        "            st.error(\"Unsupported file type.\")\n",
        "            user_text = \"\"\n",
        "        user_text = \" \".join(user_text.split()[:1000])\n",
        "    else:\n",
        "        user_text = \"\"\n",
        "\n",
        "# Word count\n",
        "if user_text:\n",
        "    word_count = len(user_text.split())\n",
        "    st.sidebar.success(f\"Word Count: {word_count}/1000\")\n",
        "\n",
        "# Sidebar: Analysis Options\n",
        "st.sidebar.header(\"Analysis Options\")\n",
        "run_sentiment = st.sidebar.checkbox(\"Sentiment Analysis\", value=True)\n",
        "run_sentiment_over_time = st.sidebar.checkbox(\"Track Sentiment Over Time\", value=True)\n",
        "run_readability = st.sidebar.checkbox(\"Readability Metrics\", value=True)\n",
        "run_summary = st.sidebar.checkbox(\"Generate Summary\", value=False)\n",
        "run_emotions = st.sidebar.checkbox(\"Emotion Analysis\", value=True)\n",
        "run_emotions_overtime = st.sidebar.checkbox(\"Emotion Over Time\", value=False)\n",
        "run_semantic = st.sidebar.checkbox(\"Semantic Complexity Metrics\", value=True)\n",
        "use_bert = st.sidebar.checkbox(\"Use BERT for Semantic Complexity\", value=False)\n",
        "run_dataset = st.sidebar.checkbox(\"Compare with Dataset\", value=False)\n",
        "\n",
        "# Main content\n",
        "if user_text:\n",
        "    st.subheader(\"Input Text\")\n",
        "    st.write(user_text)\n",
        "\n",
        "    tabs = st.tabs([\n",
        "        \"Sentiment\",\n",
        "        \"Readability\",\n",
        "        \"Summary\",\n",
        "        \"Emotions\",\n",
        "        \"Semantic Complexity\",\n",
        "        \"WordCloud & Frequency\",\n",
        "        \"Dataset Comparison\"\n",
        "    ])\n",
        "\n",
        "    # ----- Tab 1: Sentiment -----\n",
        "    with tabs[0]:\n",
        "        st.header(\"Sentiment Analysis\")\n",
        "        if run_sentiment:\n",
        "            with st.spinner(\"Analyzing sentiment...\"):\n",
        "                sentiment_scores = analyze_sentiment_pretrained(user_text)\n",
        "\n",
        "            neg = sentiment_scores.get(\"negative\", 0)\n",
        "            pos = sentiment_scores.get(\"positive\", 0)\n",
        "            neu = sentiment_scores.get(\"neutral\", 0)\n",
        "            st.write(\"**Overall Sentiment:**\")\n",
        "            st.write(\n",
        "                f\"- Positive: {pos:.2f}\\n\"\n",
        "                f\"- Negative: {neg:.2f}\\n\"\n",
        "                f\"- Neutral: {neu:.2f}\\n\"\n",
        "            )\n",
        "\n",
        "            if run_sentiment_over_time:\n",
        "                st.subheader(\"Sentiment Over Time\")\n",
        "                fig_time = track_sentiment_over_time(user_text)\n",
        "                st.plotly_chart(fig_time, use_container_width=True)\n",
        "\n",
        "            st.subheader(\"Overall Sentiment Distribution\")\n",
        "            fig_sent = plot_sentiment_bar(sentiment_scores)\n",
        "            st.plotly_chart(fig_sent, use_container_width=True)\n",
        "        else:\n",
        "            st.info(\"Enable 'Sentiment Analysis' from the sidebar.\")\n",
        "\n",
        "    # ----- Tab 2: Readability -----\n",
        "    with tabs[1]:\n",
        "        st.header(\"Readability Metrics\")\n",
        "        if run_readability:\n",
        "            with st.spinner(\"Computing readability metrics...\"):\n",
        "                readability = compute_readability_scores(user_text)\n",
        "            fk_score = readability['Flesch_Kincaid_Grade']\n",
        "            dc_score = readability['Dale_Chall']\n",
        "\n",
        "            st.write(\"### Flesch-Kincaid Grade Level\")\n",
        "            st.write(f\"- **Score**: {fk_score:.2f}\")\n",
        "            if fk_score < 5:\n",
        "                level_desc = \"Elementary school level.\"\n",
        "            elif fk_score < 8:\n",
        "                level_desc = \"Middle school level.\"\n",
        "            elif fk_score < 12:\n",
        "                level_desc = \"High school level.\"\n",
        "            elif fk_score < 16:\n",
        "                level_desc = \"College-level text.\"\n",
        "            else:\n",
        "                level_desc = \"Very advanced, possibly post-graduate level.\"\n",
        "            st.write(f\"- **Interpretation**: {level_desc}\")\n",
        "            st.write(\"*Typical newspapers range around 8-10, academic papers can be 14+.*\")\n",
        "\n",
        "            st.write(\"### Dale-Chall Readability Score\")\n",
        "            st.write(f\"- **Score**: {dc_score:.2f}\")\n",
        "            if dc_score < 5:\n",
        "                dc_desc = \"Easily understandable by average 4th-grade student.\"\n",
        "            elif dc_score < 8:\n",
        "                dc_desc = \"Conversational / typical magazine-level text.\"\n",
        "            elif dc_score < 11:\n",
        "                dc_desc = \"College-level text.\"\n",
        "            else:\n",
        "                dc_desc = \"Advanced or very difficult text.\"\n",
        "            st.write(f\"- **Interpretation**: {dc_desc}\")\n",
        "            st.write(\"*Dale-Chall above ~10 indicates advanced-level reading.*\")\n",
        "        else:\n",
        "            st.info(\"Enable 'Readability Metrics' from the sidebar.\")\n",
        "\n",
        "    # ----- Tab 3: Summary -----\n",
        "    with tabs[2]:\n",
        "        st.header(\"Summary\")\n",
        "        if run_summary:\n",
        "            try:\n",
        "                with st.spinner(\"Generating summary...\"):\n",
        "                    summary_text = generate_summary(user_text)\n",
        "                st.subheader(\"Generated Summary\")\n",
        "                st.write(summary_text)\n",
        "\n",
        "                original_word_count = len(user_text.split())\n",
        "                summary_word_count = len(summary_text.split())\n",
        "                if original_word_count > 0:\n",
        "                    compression = 1 - (summary_word_count / original_word_count)\n",
        "                    st.write(\n",
        "                        f\"**Compression Ratio**: Reduced word count by \"\n",
        "                        f\"{compression*100:.1f}% (from {original_word_count} to {summary_word_count}).\"\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Summary generation failed: {e}\")\n",
        "        else:\n",
        "            st.info(\"Enable 'Generate Summary' from the sidebar.\")\n",
        "\n",
        "    # ----- Tab 4: Emotions -----\n",
        "    with tabs[3]:\n",
        "        st.header(\"Emotion Analysis\")\n",
        "        if run_emotions:\n",
        "            with st.spinner(\"Analyzing emotions...\"):\n",
        "                basic_emotions = analyze_emotions(user_text)\n",
        "                pretrained_emotions = analyze_emotions_pretrained(user_text)\n",
        "\n",
        "            st.subheader(\"Basic Keyword-based Emotion Counts\")\n",
        "            for emo, count in basic_emotions.items():\n",
        "                st.write(f\"- {emo}: {count}\")\n",
        "\n",
        "            st.subheader(\"Pretrained Emotion Scores\")\n",
        "            sorted_emotions = sorted(pretrained_emotions.items(), key=lambda x: x[1], reverse=True)\n",
        "            for emo, score in sorted_emotions:\n",
        "                st.write(f\"- {emo}: {score:.2f}\")\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "\n",
        "            if run_emotions_overtime:\n",
        "                st.subheader(\"Emotion Changes Over Time\")\n",
        "                with st.spinner(\"Analyzing emotions sentence by sentence...\"):\n",
        "                    emotions_dict = analyze_emotions_over_time(user_text)\n",
        "                fig_emotions_time = plot_emotions_over_time(emotions_dict)\n",
        "                st.plotly_chart(fig_emotions_time, use_container_width=True)\n",
        "                st.write(\"\"\"\n",
        "                    **How to Interpret**:\n",
        "                    - Each line corresponds to an emotion's intensity across consecutive sentences.\n",
        "                    - Spikes indicate where that emotion is strongly expressed.\n",
        "                    - Flat or near-zero lines suggest minimal expression of that emotion.\n",
        "                \"\"\")\n",
        "        else:\n",
        "            st.info(\"Enable 'Emotion Analysis' from the sidebar.\")\n",
        "\n",
        "    # ----- Tab 5: Semantic Complexity -----\n",
        "    with tabs[4]:\n",
        "        st.header(\"Semantic Complexity Metrics\")\n",
        "        if run_semantic:\n",
        "            with st.spinner(\"Computing semantic complexity...\"):\n",
        "                if use_bert:\n",
        "                    complexity = jacobs_semantic_complexity(user_text, method=\"bert\")\n",
        "                else:\n",
        "                    model = train_word2vec([user_text])\n",
        "                    complexity = jacobs_semantic_complexity(user_text, method=\"word2vec\", word2vec_model=model)\n",
        "            avg_sim = complexity.get(\"avg_similarity\", 0)\n",
        "            st.write(\n",
        "                f\"**Average Sentence-to-Sentence Similarity**: {avg_sim:.2f}\\n\\n\"\n",
        "                f\"**Standard Deviation**: {complexity.get('std_similarity', 0):.2f}\"\n",
        "            )\n",
        "\n",
        "            fig_sem = plot_semantic_complexity(complexity)\n",
        "            if fig_sem:\n",
        "                st.plotly_chart(fig_sem, use_container_width=True)\n",
        "                st.write(f\"\"\"\n",
        "                    **Understanding the Graph**:\n",
        "                    - **Cosine Similarity (Blue Line)** measures how similar each pair of consecutive sentences is.\n",
        "                      Values closer to 1.0 indicate near-identical meaning.\n",
        "                    - The **Red Dashed Line** is the average similarity score ({avg_sim:.2f}), a reference\n",
        "                      for above/below-average coherence.\n",
        "                    - **Dips** suggest abrupt shifts in meaning; **peaks** indicate strong continuity.\n",
        "                \"\"\")\n",
        "        else:\n",
        "            st.info(\"Enable 'Semantic Complexity Metrics' from the sidebar.\")\n",
        "\n",
        "    # ----- Tab 6: WordCloud & Frequency -----\n",
        "    with tabs[5]:\n",
        "        st.header(\"Word Cloud & Word Frequency\")\n",
        "        max_words_wc = st.slider(\"Number of words to display in the Word Cloud\",\n",
        "                                 min_value=20, max_value=300, value=100, step=10)\n",
        "        # Add a button to trigger the animated word cloud\n",
        "        animate_wc = st.button(\"Animate Word Cloud\")\n",
        "        if animate_wc:\n",
        "            fig_wc_anim = animated_wordcloud(user_text, max_words=max_words_wc)\n",
        "            st.plotly_chart(fig_wc_anim, use_container_width=True)\n",
        "        else:\n",
        "            fig_wc = plot_word_cloud(user_text, max_words=max_words_wc)\n",
        "            st.plotly_chart(fig_wc, use_container_width=True)\n",
        "\n",
        "        st.write(\"Top 10 Word Frequency\")\n",
        "        fig_freq = plot_word_frequency(user_text, top_n=10)\n",
        "        st.plotly_chart(fig_freq, use_container_width=True)\n",
        "\n",
        "    # ----- Tab 7: Dataset Comparison -----\n",
        "    with tabs[6]:\n",
        "        st.header(\"Dataset Comparison\")\n",
        "        if run_dataset:\n",
        "            dataset_path = st.text_input(\"Enter Dataset Path\", value=\"/content/dataset/1/gutenberg\")\n",
        "            if dataset_path:\n",
        "                dataset_texts, file_names = load_dataset(dataset_path)\n",
        "                if dataset_texts:\n",
        "                    st.write(\"\"\"\n",
        "                        We compare your input text to each file in the dataset by computing\n",
        "                        average Word2Vec embeddings and measuring cosine similarity.\n",
        "                        Scores near 1.0 indicate strong similarity, while near 0.0 indicates dissimilar text.\n",
        "                    \"\"\")\n",
        "                    model = train_word2vec(dataset_texts + [user_text])\n",
        "                    similarity_scores = compute_similarity(user_text, dataset_texts, model)\n",
        "\n",
        "                    fig_sim = plot_similarity(similarity_scores, file_names)\n",
        "                    st.plotly_chart(fig_sim, use_container_width=True)\n",
        "\n",
        "                    st.write(\"\"\"\n",
        "                        **t-SNE Visualization**:\n",
        "                        This plot shows a 2D projection of the Word2Vec embeddings. It helps us see\n",
        "                        how words are grouped in semantic space.\n",
        "                    \"\"\")\n",
        "                    fig_tsne = plot_tsne(model)\n",
        "                    st.plotly_chart(fig_tsne, use_container_width=True)\n",
        "                else:\n",
        "                    st.warning(\"No dataset files to compare.\")\n",
        "        else:\n",
        "            st.info(\"Enable 'Compare with Dataset' from the sidebar to see dataset comparison.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please enter some text, a URL, or upload a file (limited to 1000 words) from the sidebar to begin analysis.\")\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <div class=\"footer\">\n",
        "        Advanced Text Analysis Dashboard &copy; 2025 | Developed with Streamlit\n",
        "    </div>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfRVqGsLjjzX",
        "outputId": "c3e2cace-a927-4f72-943d-95d026e52c65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "!ngrok config add-authtoken 2tfpVSURCrI2ZyYT3m7uZjGG0NV_5mC1356bAs4tfQrthVMWA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_BGas2yMQfD",
        "outputId": "6f8e1963-9099-4a4f-b789-2d779c55e14e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "time.sleep(5)  # Give it a few seconds to close\n",
        "\n",
        "# Create an HTTP tunnel on port 8501\n",
        "public_url = ngrok.connect(8501, proto=\"http\")\n",
        "print(\"Streamlit app running at:\", public_url)\n",
        "\n",
        "# Run Streamlit in the background\n",
        "os.system(\"streamlit run app.py &\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTu8J-XwLTkI",
        "outputId": "713be57a-b272-4792-a0fc-c733b47d7e19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app running at: NgrokTunnel: \"https://f900-35-204-154-139.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0Y9jtL_LrFL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}